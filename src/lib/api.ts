import {
    ChatRequest,
    ChatResponse,
    ModelId,
    MODELS,
} from "./types";

// â”€â”€â”€ Provider-specific call stubs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Call OpenAI Chat Completions API (gpt-4o).
 * In production, replace the mock with a real fetch to
 * https://api.openai.com/v1/chat/completions
 */
async function callOpenAI(req: ChatRequest): Promise<ChatResponse> {
    // --- MOCK IMPLEMENTATION ---
    await simulateLatency();
    const lastMessage = req.messages[req.messages.length - 1];

    if (req.model === "gpt-5-thinking") {
        return {
            content: `**[GPT-5 Thinking Mock]**\n\n> **Thinking Process** ğŸ’­\n> 1. Analyzing request: "${lastMessage.content}"\n> 2. Accessing knowledge base...\n> 3. Formulating comprehensive answer...\n\n(Here is the final output from GPT-5 with reasoning capabilities.)`,
        };
    }

    const modelName = req.model === "gpt-5" ? "GPT-5 (Preview)" : "GPT-4o";
    return {
        content: `**[${modelName} Mock]**\n\nã‚ãªãŸã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€Œ${lastMessage.content}ã€ã‚’å—ã‘å–ã‚Šã¾ã—ãŸã€‚\n\nå®Ÿéš›ã®APIã‚­ãƒ¼ã‚’è¨­å®šã™ã‚Œã°ã€OpenAIã®${modelName}ãƒ¢ãƒ‡ãƒ«ãŒå¿œç­”ã—ã¾ã™ã€‚`,
    };
    // --- PRODUCTION EXAMPLE ---
    // const res = await fetch("https://api.openai.com/v1/chat/completions", {
    //   method: "POST",
    //   headers: {
    //     "Content-Type": "application/json",
    //     Authorization: `Bearer ${req.apiKey}`,
    //   },
    //   body: JSON.stringify({
    //     model: "gpt-4o",
    //     messages: req.messages,
    //   }),
    // });
    // const json = await res.json();
    // return { content: json.choices[0].message.content };
}

/**
 * Call Google Gemini API (gemini-1.5-pro).
 */
async function callGemini(req: ChatRequest): Promise<ChatResponse> {
    await simulateLatency();
    const lastMessage = req.messages[req.messages.length - 1];
    const modelName =
        req.model === "gemini-3.0-pro" ? "Gemini 3.0 Pro" : "Gemini 1.5 Pro";
    return {
        content: `**[${modelName} Mock]**\n\nã€Œ${lastMessage.content}ã€ã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã¾ã—ãŸã€‚\n\nå®Ÿéš›ã®APIã‚­ãƒ¼ã‚’è¨­å®šã™ã‚Œã°ã€Google ${modelName}ãŒå¿œç­”ã—ã¾ã™ã€‚\n\n- ãƒã‚¤ãƒ³ãƒˆ1: ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã™\n- ãƒã‚¤ãƒ³ãƒˆ2: Markdownã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™`,
    };
}

/**
 * Call Anthropic Messages API (claude-3-5-sonnet).
 */
async function callAnthropic(req: ChatRequest): Promise<ChatResponse> {
    await simulateLatency();
    const lastMessage = req.messages[req.messages.length - 1];
    const modelName =
        req.model === "claude-4-6-sonnet"
            ? "Claude 4.6 Sonnet"
            : "Claude 3.5 Sonnet";
    return {
        content: `**[${modelName} Mock]**\n\nã”è³ªå•ã€Œ${lastMessage.content}ã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚\n\n> ã“ã‚Œã¯ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã™ã€‚å®Ÿéš›ã®APIã‚­ãƒ¼ã‚’è¨­å®šã™ã‚Œã°ã€Anthropic ${modelName} ãŒå¿œç­”ã—ã¾ã™ã€‚\n\n\`\`\`python\nprint("Hello from ${modelName}!")\n\`\`\``,
    };
}

/**
 * Call OpenAI DALLÂ·E 3 image generation API.
 */
async function callDallE(req: ChatRequest): Promise<ChatResponse> {
    await simulateLatency();
    const lastMessage = req.messages[req.messages.length - 1];
    return {
        content: `DALLÂ·E 3 ã§ã€Œ${lastMessage.content}ã€ã®ç”»åƒã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚`,
        imageUrl:
            "https://images.unsplash.com/photo-1676299081847-824916de030a?w=512&h=512&fit=crop",
    };
    // --- PRODUCTION EXAMPLE ---
    // const res = await fetch("https://api.openai.com/v1/images/generations", {
    //   method: "POST",
    //   headers: {
    //     "Content-Type": "application/json",
    //     Authorization: `Bearer ${req.apiKey}`,
    //   },
    //   body: JSON.stringify({
    //     model: "dall-e-3",
    //     prompt: lastMessage.content,
    //     n: 1,
    //     size: "1024x1024",
    //   }),
    // });
    // const json = await res.json();
    // return { content: "ç”»åƒã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚", imageUrl: json.data[0].url };
}

/**
 * Call Nano Banana image generation API (hypothetical).
 */
async function callNanoBanana(req: ChatRequest): Promise<ChatResponse> {
    await simulateLatency();
    const lastMessage = req.messages[req.messages.length - 1];
    return {
        content: `Nano Banana ã§ã€Œ${lastMessage.content}ã€ã®ç”»åƒã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚`,
        imageUrl:
            "https://images.unsplash.com/photo-1543466835-00a7907e9de1?w=512&h=512&fit=crop",
    };
}

// â”€â”€â”€ Router â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Route the request to the appropriate provider handler based on the
 * selected model. This is the single entry point used by the UI.
 */
export async function sendChatRequest(
    req: ChatRequest
): Promise<ChatResponse> {
    const model = MODELS.find((m) => m.id === req.model);
    if (!model) {
        throw new Error(`Unknown model: ${req.model}`);
    }

    const handlers: Record<ModelId, (r: ChatRequest) => Promise<ChatResponse>> = {
        "gpt-4o": callOpenAI,
        "gpt-5": callOpenAI,
        "gpt-5-thinking": callOpenAI,
        "gemini-1.5-pro": callGemini,
        "gemini-3.0-pro": callGemini,
        "claude-3-5-sonnet": callAnthropic,
        "claude-4-6-sonnet": callAnthropic,
        "dall-e-3": callDallE,
        "nano-banana": callNanoBanana,
    };

    return handlers[req.model](req);
}

// â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function simulateLatency(): Promise<void> {
    return new Promise((resolve) =>
        setTimeout(resolve, 800 + Math.random() * 700)
    );
}
